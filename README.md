These different files of code serve a purpose detailed in the comments.
In general, I used the ROOT package for Python to extract and analyse data, as CERN stores its data from detectors and simulations in .root files to read only the necessary data very efficiently. This is because each .root file can carry information on billions of particles, and have multiple pieces of data on each one, such as transverse momentum. They store data in a structure called a TTree. You may only want to use one certain piece of data, which is why we use .root files so we can choose the branches we read, instead of wasting computational power reading the whole file.

All these files reconstruct jets of particles that flow into the ATLAS detector due to how particles that enter the detector "shower down" into more and more particles, as the particles progressively lose energy to mass. I used data either from MC simulation or directly from ATLAS. I am utilizing the AntiKt Jet Clustering Algorithm within the radius R = 0.4. This tracks jets of particles based on each particle's momentum vector and its pseudorapidity, and groups the particles up into jets. My main general function was to match 'reco' (reconstructed) jets to truth jets that we know exist because of the MC simulation, and evaluating how accurate this reconstruction algorithm can be.

In each of the files, I go into depth comparing the effectiveness of the reconstruction in in 'hard-scatter' particles, a.k.a. the primary verteces (particles coming out of 'big' collisions at CERN, they produce the famous Higgs Boson), or in 'pileup' particles (the 'rest' of the particles, they do not have as much energy nor, naturally, do they produce the Higgs Boson). My study explored the extent of lesser effectivity of jet clustering for 'pileup' particles.
